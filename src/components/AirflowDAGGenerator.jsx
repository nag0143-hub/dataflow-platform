import { FLAT_FILE_PLATFORMS } from "@/components/JobSpecExport";

function isFlat(platform) {
  return FLAT_FILE_PLATFORMS.includes(platform);
}

function resolveOperator(srcPlatform, tgtPlatform, override) {
  if (override === "custom_template") return "custom_template";
  if (override && override !== "auto") return override;
  return (isFlat(srcPlatform) || isFlat(tgtPlatform)) ? "PythonOperator" : "SparkSubmitOperator";
}

function buildSensorBlock(job, dagId) {
  const sensorType = job.event_sensor_type || "file_watcher";
  const cfg = job.event_config || {};
  const poke = cfg.poll_interval || "60";
  const timeout = (cfg.timeout_hours || "24") * 3600;

  const sensorMap = {
    file_watcher: {
      cls: "FileSensor",
      imp: "from airflow.sensors.filesystem import FileSensor",
      args: `filepath="${cfg.watch_path || "/data/inbound/*"}",\n        fs_conn_id="fs_default",`,
    },
    s3_event: (() => {
      const raw = cfg.watch_path || "s3://my-bucket/prefix/";
      const match = raw.match(/^s3:\/\/([^/]+)\/?(.*)$/);
      const bucket = match ? match[1] : "my-bucket";
      const prefix = match ? match[2] : "prefix/*";
      return {
        cls: "S3KeySensor",
        imp: "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor",
        args: `bucket_name="${bucket}",\n        bucket_key="${prefix || "*"}",\n        wildcard_match=True,\n        aws_conn_id="aws_default",`,
      };
    })(),
    db_sensor: {
      cls: "SqlSensor",
      imp: "from airflow.providers.common.sql.sensors.sql import SqlSensor",
      args: `conn_id="source_connection",\n        sql="${(cfg.sql_condition || "SELECT 1").replace(/"/g, '\\"')}",`,
    },
    sftp_sensor: {
      cls: "SFTPSensor",
      imp: "from airflow.providers.sftp.sensors.sftp import SFTPSensor",
      args: `path="${cfg.watch_path || "/inbound/"}",\n        sftp_conn_id="sftp_default",`,
    },
    api_webhook: {
      cls: "HttpSensor",
      imp: "from airflow.providers.http.sensors.http import HttpSensor",
      args: `http_conn_id="http_default",\n        endpoint="/api/webhooks/pipeline/${dagId}/trigger",\n        method="GET",\n        response_check=lambda response: response.json().get("ready", False),`,
    },
    upstream_job: {
      cls: "ExternalTaskSensor",
      imp: "from airflow.sensors.external_task import ExternalTaskSensor",
      args: `external_dag_id="${cfg.upstream_job || "upstream_dag"}",\n        external_task_id=None,\n        allowed_states=["success"],`,
    },
  };

  const sensor = sensorMap[sensorType] || sensorMap.file_watcher;

  return {
    importLine: sensor.imp,
    block: `
    wait_for_trigger = ${sensor.cls}(
        task_id="wait_for_trigger",
        ${sensor.args}
        poke_interval=${poke},
        timeout=${timeout},
        mode="poke",
    )`,
  };
}

export function generateAirflowDAG(job, connections) {
  const sourceConn = connections.find(c => c.id === job.source_connection_id);
  const targetConn = connections.find(c => c.id === job.target_connection_id);
  const srcPlatform = sourceConn?.platform || "";
  const tgtPlatform = targetConn?.platform || "";

  const operatorOverride = job.operator_type || "auto";
  const resolvedOp = resolveOperator(srcPlatform, tgtPlatform, operatorOverride);
  const dagId = `dataflow__${(job.name || "pipeline").replace(/[^a-zA-Z0-9]/g, "_").toLowerCase()}`;
  const datasets = job.selected_datasets || job.selected_objects || [];

  if (resolvedOp === "custom_template" && job.custom_template) {
    const rendered = job.custom_template
      .replace(/\{\{\s*dag_id\s*\}\}/g, dagId)
      .replace(/\{\{\s*schedule_interval\s*\}\}/g, job.schedule_type === "manual" ? "None" : `"${job.cron_expression || "0 6 * * *"}"`)
      .replace(/\{\{\s*source\s*\}\}/g, sourceConn?.name || job.source_connection_id || "")
      .replace(/\{\{\s*target\s*\}\}/g, targetConn?.name || job.target_connection_id || "")
      .replace(/\{\{\s*datasets\s*\}\}/g, JSON.stringify(datasets.map(d => `${d.schema}.${d.table}`)));

    return `# Auto-generated by DataFlow (Custom Template) — ${new Date().toISOString()}
# Pipeline: ${job.name}
# Source: ${sourceConn?.name || job.source_connection_id} (${srcPlatform})
# Target: ${targetConn?.name || job.target_connection_id} (${tgtPlatform})

${rendered}`;
  }

  const isEventDriven = job.schedule_type === "event_driven";
  const cronExpr = job.schedule_type === "manual" || isEventDriven ? null
    : job.cron_expression || (
      job.schedule_type === "hourly" ? "0 * * * *"
      : job.schedule_type === "daily" ? "0 0 * * *"
      : job.schedule_type === "weekly" ? "0 0 * * 0"
      : job.schedule_type === "monthly" ? "0 0 1 * *"
      : job.schedule_type === "quarterly" ? "0 0 1 1,4,7,10 *"
      : job.schedule_type === "yearly" ? "0 0 1 1 *"
      : null
    );
  const scheduleStr = cronExpr ? `"${cronExpr}"` : "None";

  const useSpark = resolvedOp === "SparkSubmitOperator";
  const sparkCfg = job.spark_config || {};
  const pythonCfg = job.python_config || {};

  const taskDefs = datasets.map(ds => {
    const taskId = `${ds.schema}__${ds.table}`.replace(/[^a-zA-Z0-9_]/g, "_").toLowerCase();
    const loadMethod = ds.load_method || job.load_method || "append";

    if (useSpark) {
      const execMem = sparkCfg.executor_memory || "4g";
      const execCores = sparkCfg.executor_cores || "2";
      const driverMem = sparkCfg.driver_memory || "2g";
      const app = sparkCfg.application || "dataflow_spark_ingestion.py";
      const pyFiles = (sparkCfg.py_files || "dataflow_utils.zip").split(",").map(f => f.trim()).filter(Boolean);

      return `
    # Task: ${ds.schema}.${ds.table} [SparkSubmitOperator - PySpark]
    task_${taskId} = SparkSubmitOperator(
        task_id="${taskId}",
        application="${app}",
        application_args=[
            "--schema", "${ds.schema}",
            "--table", "${ds.table}",
            "--target_path", "${ds.target_path || ""}",
            "--load_method", "${loadMethod}",
            "--filter_query", "${(ds.filter_query || "").replace(/"/g, '\\"')}",
            "--incremental_column", "${ds.incremental_column || ""}",
        ],
        conf={
            "spark.executor.memory": "${execMem}",
            "spark.executor.cores": "${execCores}",
            "spark.driver.memory": "${driverMem}",
        },
        py_files=${JSON.stringify(pyFiles)},
        retries=${job.retry_config?.max_retries ?? 3},
        retry_delay=timedelta(seconds=${job.retry_config?.retry_delay_seconds ?? 60}),
    )`;
    } else {
      const callable = pythonCfg.callable || "run_ingestion";

      return `
    # Task: ${ds.schema}.${ds.table} [PythonOperator]
    task_${taskId} = PythonOperator(
        task_id="${taskId}",
        python_callable=${callable},
        op_kwargs={
            "source_platform": "${srcPlatform}",
            "target_platform": "${tgtPlatform}",
            "schema": "${ds.schema}",
            "table": "${ds.table}",
            "target_path": "${ds.target_path || ""}",
            "load_method": "${loadMethod}",
            "filter_query": "${(ds.filter_query || "").replace(/"/g, '\\"')}",
            "incremental_column": "${ds.incremental_column || ""}",
        },
        retries=${job.retry_config?.max_retries ?? 3},
        retry_delay=timedelta(seconds=${job.retry_config?.retry_delay_seconds ?? 60}),
    )`;
    }
  });

  const taskIds = datasets.map(ds =>
    `task_${ds.schema}__${ds.table}`.replace(/[^a-zA-Z0-9_]/g, "_").toLowerCase()
  );

  let sensorImport = "";
  let sensorBlock = "";
  let dependencyChain;

  if (isEventDriven) {
    const sensor = buildSensorBlock(job, dagId);
    sensorImport = `\n${sensor.importLine}`;
    sensorBlock = sensor.block;

    if (taskIds.length > 1) {
      dependencyChain = `\n    # Sensor triggers all dataset tasks in parallel\n    wait_for_trigger >> [${taskIds.join(", ")}]\n`;
    } else if (taskIds.length === 1) {
      dependencyChain = `\n    wait_for_trigger >> ${taskIds[0]}\n`;
    } else {
      dependencyChain = `\n    wait_for_trigger\n`;
    }
  } else {
    if (taskIds.length > 1) {
      dependencyChain = `\n    # All dataset tasks run in parallel\n    [${taskIds.join(", ")}]\n`;
    } else if (taskIds.length === 1) {
      dependencyChain = `\n    # Single task\n    ${taskIds[0]}\n`;
    } else {
      dependencyChain = "\n    pass\n";
    }
  }

  const callable = pythonCfg.callable || "run_ingestion";
  const moduleImportLine = !useSpark && pythonCfg.module
    ? `\nfrom ${pythonCfg.module} import ${callable}`
    : !useSpark
    ? `\n\ndef ${callable}(**kwargs):\n    """Auto-generated stub — replace with your ingestion logic."""\n    print(f"Running ingestion: {kwargs}")\n`
    : "";

  const operatorImport = useSpark
    ? `from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator`
    : `from airflow.operators.python import PythonOperator${moduleImportLine}`;

  const triggerTag = isEventDriven ? "event_driven" : (job.schedule_type || "manual");
  const opTag = useSpark ? "pyspark" : "python";

  return `# Auto-generated by DataFlow — ${new Date().toISOString()}
# Pipeline: ${job.name}
# Source: ${sourceConn?.name || job.source_connection_id} (${srcPlatform})
# Target: ${targetConn?.name || job.target_connection_id} (${tgtPlatform})
# Trigger: ${triggerTag}
# Operator: ${resolvedOp}

from datetime import datetime, timedelta
from airflow import DAG
${operatorImport}${sensorImport}

default_args = {
    "owner": "${job.assignment_group || "dataflow"}",
    "depends_on_past": ${job.depends_on_past ? "True" : "False"},
    "email_on_failure": ${job.email ? "True" : "False"},
    "email_on_retry": ${job.email_on_retry ? "True" : "False"},
    ${job.email ? `"email": ["${job.email}"],` : ""}
    "retries": ${job.retry_config?.max_retries ?? 3},
    "retry_delay": timedelta(seconds=${job.retry_config?.retry_delay_seconds ?? 60}),${job.retry_config?.retry_exponential_backoff ? `\n    "retry_exponential_backoff": True,` : ""}${job.wait_for_downstream ? `\n    "wait_for_downstream": True,` : ""}${job.sla_seconds ? `\n    "sla": timedelta(seconds=${job.sla_seconds}),` : ""}${job.execution_timeout ? `\n    "execution_timeout": timedelta(seconds=${job.execution_timeout}),` : ""}
}

with DAG(
    dag_id="${dagId}",
    default_args=default_args,
    description="${(job.description || "").replace(/"/g, '\\"')}",
    schedule_interval=${scheduleStr},
    start_date=datetime(${(() => { const d = job.start_date || "2024-01-01"; const p = d.split("-"); return `${p[0]}, ${parseInt(p[1])}, ${parseInt(p[2])}`; })()}),
    ${job.end_date ? `end_date=datetime(${(() => { const p = job.end_date.split("-"); return `${p[0]}, ${parseInt(p[1])}, ${parseInt(p[2])}`; })()}),` : ""}catchup=${job.catchup ? "True" : "False"},${job.use_custom_calendar ? `\n    # Calendar Timetable: include=${job.include_calendar_id || "none"}, exclude=${job.exclude_calendar_id || "none"}${job.custom_include_dates ? `\n    # Custom include dates: ${job.custom_include_dates}` : ""}${job.custom_exclude_dates ? `\n    # Custom exclude dates: ${job.custom_exclude_dates}` : ""}` : ""}
    tags=["dataflow", "${opTag}", "${triggerTag}"${(job.dag_tags || []).length > 0 ? `, ${job.dag_tags.map(t => `"${t}"`).join(", ")}` : ""}],${job.concurrency && job.concurrency !== 16 ? `\n    concurrency=${job.concurrency},` : ""}${job.max_active_runs > 1 ? `\n    max_active_runs=${job.max_active_runs},` : ""}${job.dagrun_timeout > 0 ? `\n    dagrun_timeout=timedelta(minutes=${job.dagrun_timeout}),` : ""}${job.is_paused_upon_creation !== false ? "" : `\n    is_paused_upon_creation=False,`}
) as dag:
${sensorBlock}${taskDefs.join("\n")}
${dependencyChain}`;
}

function resolveSchedule(job) {
  const st = job.schedule_type || "manual";
  if (st === "manual") return "@once";
  if (st === "hourly") return job.cron_expression || "@hourly";
  if (st === "daily") return job.cron_expression || "@daily";
  if (st === "weekly") return job.cron_expression || "@weekly";
  if (st === "monthly") return job.cron_expression || "@monthly";
  if (st === "quarterly") return job.cron_expression || "0 0 1 1,4,7,10 *";
  if (st === "yearly") return job.cron_expression || "@yearly";
  if (st === "event_driven") return null;
  if (st === "every_minutes") return job.cron_expression || "*/15 * * * *";
  if (st === "every_hours") return job.cron_expression || "0 */2 * * *";
  if (st === "custom" && job.cron_expression) return job.cron_expression;
  return "@once";
}

function sensorCallableName(platform) {
  const map = {
    sftp: "sftp_file_exists",
    local_fs: "local_file_exists",
    nas: "nas_file_exists",
    flat_file_delimited: "adls_file_exists",
    flat_file_fixed_width: "adls_file_exists",
    cobol_ebcdic: "adls_file_exists",
  };
  return map[platform] || "adls_file_exists";
}

function dagFactoryYaml(obj, indent = 0) {
  const pad = "  ".repeat(indent);
  if (obj === null || obj === undefined) return "null";
  if (typeof obj === "boolean") return obj ? "true" : "false";
  if (typeof obj === "number") return String(obj);
  if (typeof obj === "string") {
    if (/[\n#\[\]{},&*?|<>=!`]/.test(obj) || obj === "" || /^(true|false|null|yes|no)$/i.test(obj))
      return `"${obj.replace(/"/g, '\\"')}"`;
    if (obj.includes(": ") || obj.startsWith(":"))
      return `"${obj.replace(/"/g, '\\"')}"`;
    return obj;
  }
  if (Array.isArray(obj)) {
    if (obj.length === 0) return "[]";
    return obj.map(item => {
      const val = dagFactoryYaml(item, indent + 1);
      if (typeof item === "object" && item !== null && !Array.isArray(item)) {
        const lines = val.split("\n");
        return `${pad}- ${lines[0]}\n${lines.slice(1).join("\n")}`;
      }
      return `${pad}- ${val}`;
    }).join("\n");
  }
  if (typeof obj === "object") {
    const keys = Object.keys(obj);
    if (keys.length === 0) return "{}";
    return keys.map(k => {
      const val = obj[k];
      if (val !== null && typeof val === "object" && !Array.isArray(val) && Object.keys(val).length > 0)
        return `${pad}${k}:\n${dagFactoryYaml(val, indent + 1)}`;
      if (Array.isArray(val) && val.length > 0 && typeof val[0] === "object")
        return `${pad}${k}:\n${dagFactoryYaml(val, indent + 1)}`;
      if (Array.isArray(val) && val.length > 0)
        return `${pad}${k}:\n${val.map(v => `${pad}  - ${dagFactoryYaml(v, 0)}`).join("\n")}`;
      return `${pad}${k}: ${dagFactoryYaml(val, indent)}`;
    }).join("\n");
  }
  return String(obj);
}

export function generateAirflowDAGYaml(job, connections) {
  const sourceConn = connections.find(c => c.id === job.source_connection_id);
  const targetConn = connections.find(c => c.id === job.target_connection_id);
  const srcPlatform = sourceConn?.platform || "";
  const tgtPlatform = targetConn?.platform || "";
  const isFlatFile = isFlat(srcPlatform);

  const dagId = `dataflow__${(job.name || "pipeline").replace(/[^a-zA-Z0-9]/g, "_").toLowerCase()}`;
  const pipelineNameClean = (job.name || "pipeline").replace(/[^a-z0-9_]/gi, "_").toLowerCase();
  const datasets = job.selected_datasets || job.selected_objects || [];
  const basePath = (job.dag_callable_base_path || "/data/dags/").replace(/\/+$/, "");
  const callableFile = `${basePath}/${pipelineNameClean}_tasks.py`;
  const sparkApp = `${basePath}/${pipelineNameClean}_spark.py`;
  const schedule = resolveSchedule(job);
  const columnMappings = job.column_mappings || {};

  const defaultArgs = {
    owner: job.assignment_group || "data-eng",
    email_on_failure: !!job.email,
    email_on_retry: !!job.email_on_retry,
    retries: job.retry_config?.max_retries ?? 2,
    retry_delay_sec: job.retry_config?.retry_delay_seconds ?? 60,
    start_date: job.start_date || "2024-01-01",
    depends_on_past: !!job.depends_on_past,
    wait_for_downstream: !!job.wait_for_downstream,
  };
  if (job.email) {
    defaultArgs.email = [job.email];
  }
  if (job.retry_config?.retry_exponential_backoff) {
    defaultArgs.retry_exponential_backoff = true;
  }
  if (job.execution_timeout) {
    defaultArgs.execution_timeout_sec = job.execution_timeout;
  }
  if (job.sla_seconds) {
    defaultArgs.sla_sec = job.sla_seconds;
  }

  const tasks = {};

  if (isFlatFile) {
    const pokeInterval = parseInt(job.event_config?.poll_interval) || 60;
    const timeoutHours = parseInt(job.event_config?.timeout_hours) || 10;

    tasks["wait_for_source_file"] = {
      operator: "airflow.providers.standard.sensors.python.PythonSensor",
      python_callable_name: sensorCallableName(srcPlatform),
      python_callable_file: callableFile,
      poke_interval: pokeInterval,
      timeout: timeoutHours * 3600,
      soft_fail: false,
      mode: "reschedule",
    };

    if (datasets.length > 0) {
      datasets.forEach((ds, idx) => {
        const dsKey = `${ds.schema || "default"}__${ds.table || `file_${idx}`}`.replace(/[^a-zA-Z0-9_]/g, "_").toLowerCase();
        const ingestTaskId = `ingest_${dsKey}`;
        const loadMethod = ds.load_method || job.load_method || "append";

        let sourcePath = job.file_source_folder || "/data/inbound/";
        if (job.file_source_mode === "wildcard") {
          sourcePath = job.file_source_wildcard || "/data/inbound/*";
        } else if (job.file_source_mode === "file_list") {
          sourcePath = job.file_source_list?.[0]?.file_name || "input_file";
        }

        tasks[ingestTaskId] = {
          operator: "airflow.providers.standard.operators.python.PythonOperator",
          python_callable_name: "ingest_function",
          python_callable_file: callableFile,
          op_args: [sourcePath, dsKey, loadMethod, ds.target_path || ""],
          dependencies: ["wait_for_source_file"],
        };

        const dsMapKey = `${ds.schema}.${ds.table}`;
        const hasMapping = columnMappings[dsMapKey] && Array.isArray(columnMappings[dsMapKey]) && columnMappings[dsMapKey].length > 0;

        if (hasMapping) {
          const transformTaskId = `transform_${dsKey}`;
          const appArgs = [
            "--schema", ds.schema || "default",
            "--table", ds.table || `file_${idx}`,
            "--load_method", loadMethod,
            "--mapping_file", `${basePath}/mappings/${pipelineNameClean}_${dsKey}_mapping.json`,
          ];
          if (ds.target_path) {
            appArgs.push("--target_path", ds.target_path);
          }

          tasks[transformTaskId] = {
            operator: "airflow.providers.apache.spark.operators.spark_submit.SparkSubmitOperator",
            application: sparkApp,
            application_args: appArgs,
            conf: {
              "spark.executor.memory": "4g",
              "spark.executor.cores": "2",
              "spark.driver.memory": "2g",
            },
            dependencies: [ingestTaskId],
          };
        }
      });
    } else {
      tasks["ingest_files"] = {
        operator: "airflow.providers.standard.operators.python.PythonOperator",
        python_callable_name: "ingest_function",
        python_callable_file: callableFile,
        op_args: [
          job.file_source_folder || job.file_source_wildcard || "/data/inbound/",
          pipelineNameClean,
          job.load_method || "append",
        ],
        dependencies: ["wait_for_source_file"],
      };
    }
  } else {
    datasets.forEach((ds, idx) => {
      const dsKey = `${ds.schema || "public"}__${ds.table || `table_${idx}`}`.replace(/[^a-zA-Z0-9_]/g, "_").toLowerCase();
      const taskId = `extract_${dsKey}`;
      const loadMethod = ds.load_method || job.load_method || "append";

      const appArgs = [
        "--source_platform", srcPlatform,
        "--target_platform", tgtPlatform,
        "--schema", ds.schema || "public",
        "--table", ds.table || `table_${idx}`,
        "--load_method", loadMethod,
      ];
      if (ds.filter_query) {
        appArgs.push("--filter_query", ds.filter_query);
      }
      if (ds.incremental_column) {
        appArgs.push("--incremental_column", ds.incremental_column);
      }
      if (ds.target_path) {
        appArgs.push("--target_path", ds.target_path);
      }

      tasks[taskId] = {
        operator: "airflow.providers.apache.spark.operators.spark_submit.SparkSubmitOperator",
        application: sparkApp,
        application_args: appArgs,
        conf: {
          "spark.executor.memory": "4g",
          "spark.executor.cores": "2",
          "spark.driver.memory": "2g",
        },
      };
    });
  }

  const dagDef = {
    default_args: defaultArgs,
    schedule: schedule,
    catchup: !!job.catchup,
    description: job.description || `DataFlow pipeline: ${job.name || "untitled"}`,
    is_paused_upon_creation: job.is_paused_upon_creation !== false,
    tags: ["dataflow", ...(job.dag_tags || [])],
    tasks: tasks,
  };
  if (job.max_active_runs && job.max_active_runs > 1) {
    dagDef.max_active_runs = job.max_active_runs;
  }
  if (job.concurrency && job.concurrency !== 16) {
    dagDef.concurrency = job.concurrency;
  }
  if (job.dagrun_timeout && job.dagrun_timeout > 0) {
    dagDef.dagrun_timeout_sec = job.dagrun_timeout * 60;
  }
  if (job.end_date) {
    dagDef.end_date = job.end_date;
  }
  if (job.pool) {
    dagDef.pool = job.pool;
  }
  if (job.priority_weight && job.priority_weight > 1) {
    dagDef.priority_weight = job.priority_weight;
  }
  if (job.use_custom_calendar) {
    dagDef.calendar_timetable = {
      include_calendar: job.include_calendar_id || null,
      exclude_calendar: job.exclude_calendar_id || null,
    };
    if (job.custom_include_dates) dagDef.calendar_timetable.custom_include_dates = job.custom_include_dates;
    if (job.custom_exclude_dates) dagDef.calendar_timetable.custom_exclude_dates = job.custom_exclude_dates;
  }
  const dagObj = {};
  dagObj[dagId] = dagDef;

  const lines = [];
  lines.push(`# dag-factory DAG Definition — Generated by DataFlow`);
  lines.push(`# Pipeline: ${job.name || "untitled"}`);
  lines.push(`# Source: ${sourceConn?.name || "unknown"} (${srcPlatform})`);
  lines.push(`# Target: ${targetConn?.name || "unknown"} (${tgtPlatform})`);
  lines.push(`# Generated: ${new Date().toISOString()}`);
  lines.push(``);
  lines.push(dagFactoryYaml(dagObj));

  return lines.join("\n");
}
